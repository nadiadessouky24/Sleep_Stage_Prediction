{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c729dc01-3c81-4abb-a011-a1f56843219f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./env/lib/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./env/lib/python3.13/site-packages (from scikit-learn) (2.2.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./env/lib/python3.13/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./env/lib/python3.13/site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./env/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pandas in ./env/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./env/lib/python3.13/site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./env/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./env/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./env/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "770cd193-58f6-49bd-ba31-552e69c2cc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adf54c27-2729-41cf-bf1a-f394d7c70c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the txt files to csv files\n",
    "directories = ['heart_rate','motion','steps','labels']\n",
    "\n",
    "for d in directories:\n",
    "    for name in os.listdir(d):\n",
    "        with open(os.path.join(d, name)) as f:\n",
    "             file_path = os.path.join(d, name)\n",
    "             dir_name = f\"{d}_csv\"\n",
    "             os.makedirs(dir_name,exist_ok=True)\n",
    "             df1 = pd.read_csv(file_path)\n",
    "             csv_file_path = os.path.join(dir_name, f\"{os.path.splitext(name)[0]}\")\n",
    "             df1.to_csv(csv_file_path + '.csv', \n",
    "                      index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8381aa6c-4a52-470a-ade8-2fa6b4090ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #sanity check \n",
    "# directories = ['steps_csv', 'heart_rate_csv', 'labels_csv', 'motion_csv']\n",
    "# counters = {'steps_csv': 0, 'heart_rate_csv': 0, 'labels_csv': 0, 'motion_csv': 0}\n",
    "# for d in directories:\n",
    "#     for name in os.listdir(d):\n",
    "#         counters[d] += 1\n",
    "\n",
    "# for key, value in counters.items():\n",
    "#     print(f\"{key}: {value} files found\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecfb42df-ea2b-4d52-9617-70a8846d3b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"motion_csv/2598705_acceleration.csv\"\n",
    "# with open(file_path, 'r') as f:\n",
    "#     for _ in range(5):\n",
    "#         # print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ad75fa1-40dd-455f-9300-c01e9fed9a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outliers using IQR Method \n",
    "count = {'steps_csv':0, 'heart_rate_csv':0, 'labels_csv':0, 'motion_csv':0}\n",
    "def outliers(input_folder,threshold):\n",
    "     df_outliers = []\n",
    "     for name in os.listdir(input_folder):\n",
    "        if not name.lower().endswith('.csv'):\n",
    "            continue\n",
    "        \n",
    "        file_path = os.path.join(input_folder, name)\n",
    "\n",
    "        if (input_folder == 'motion_csv'):\n",
    "            df = pd.read_csv(file_path, header=None, sep=r'\\s+')\n",
    "        else:\n",
    "            df = pd.read_csv(file_path, header=None)\n",
    "            \n",
    "        value_columns = df.iloc[:, 1:]\n",
    "        \n",
    "        mask = pd.Series(True, index=df.index)\n",
    "\n",
    "        for col in value_columns.columns:\n",
    "            Q1 = value_columns[col].quantile(0.25)\n",
    "            Q3 = value_columns[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - threshold * IQR\n",
    "            upper_bound = Q3 + threshold * IQR\n",
    "            \n",
    "            mask &= (value_columns[col] >= lower_bound) & (value_columns[col] <= upper_bound)  \n",
    "            \n",
    "        df_cleaned = df[mask]\n",
    "        df_outliers.append((input_folder, name, df_cleaned)) \n",
    "        \n",
    "        count[input_folder] +=1\n",
    "\n",
    "     return df_outliers\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9966c1c3-6554-4573-95b1-8b75ef73ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize \n",
    "def normalize(df):\n",
    "    time_column = df.iloc[:, 0].reset_index(drop=True) \n",
    "    value_columns = df.iloc[:, 1:].reset_index(drop=True)  \n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaled_values = scaler.fit_transform(value_columns) \n",
    "    \n",
    "    df_scaled = pd.concat([time_column, pd.DataFrame(scaled_values, columns=value_columns.columns)], axis=1)\n",
    "    \n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bb9c5ba-f43b-4693-a670-76a322a5870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = ['steps_csv', 'heart_rate_csv', 'motion_csv']\n",
    "threshold = 1.5\n",
    "\n",
    "all_outliers = []\n",
    "normalization = []\n",
    "\n",
    "for folder in folders:\n",
    "    df_list = outliers(folder, threshold)\n",
    "    all_outliers.extend(df_list)\n",
    "\n",
    "for folder, filename, df in all_outliers:\n",
    "    # print(f\"Before normalization: {filename} | Shape: {df.shape}\")\n",
    "    result = normalize(df)\n",
    "    # print(f\"After normalization: {result.shape}\")\n",
    "    normalization.append((folder, filename, result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79273133-9c63-436e-bf47-f67ad960906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check\n",
    "# for folder, filename, df in all_outliers:\n",
    "#     print(f\"Folder: {folder} | File: {filename} | Shape after cleaning: {df.shape}\")\n",
    "\n",
    "# for folder, filename, df in normalization:\n",
    "#     print(f\"Normalized {folder}/{filename} | Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d878baa0-a627-4bc4-8ff0-8581493e29d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key,value in count.items():\n",
    "#      print(f\"{key}:{value} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80c09fe7-ddae-43d0-b52b-b6a19ba4a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #validationssss\n",
    "# count = 0\n",
    "# for (folder_out, filename_out, df_out), (folder_norm, filename_norm, df_norm) in zip(all_outliers,normalization):\n",
    "#     if (filename_out != filename_norm):\n",
    "#         print(\"files are not ordered\")\n",
    "#         break\n",
    "#     if df_out.shape != df_norm.shape:\n",
    "#         print(f\"file shape mismatch for: {filename_out}\" )\n",
    "#         break\n",
    "#     else:\n",
    "#         count +=1\n",
    "#         print(\"all files have the same shape yayayayyayaay\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9bf3416-d824-40e1-b695-200b2799a716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "431295ba-b533-4ded-b029-9d2eed641e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Number of entries in all_outliers: {len(all_outliers)}\")\n",
    "# print(f\"Number of entries in normalization: {len(normalization)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b286e82b-be1e-4c5d-8348-13b4ab1f79e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lags = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7491d1ef-2a6e-4967-9f1c-158076d8a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_subject = defaultdict(dict)\n",
    "\n",
    "for folder, name, df in normalization:\n",
    "    sid = int(name.split(\"_\", 1)[0])\n",
    "    \n",
    "    by_subject[sid][folder] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3183aa61-142f-48cf-8601-438fa5d6ffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sid, streams in by_subject.items():\n",
    "#     assert set(streams.keys()) =={'steps_csv', 'heart_rate_csv', 'motion_csv'}, \\\n",
    "#         f\"Subject {sid} missing a stream: {streams}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "785f2ce6-343d-4c49-88c4-6fc59dc8b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by_subject[3997827]['heart_rate_csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d2f85d6-35ac-4887-afaa-d76b056404ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "\n",
    "for path in glob.glob(\"labels_csv/*_labeled_sleep.csv\"):\n",
    "        sid = int(path.split(\"/\")[-1].split(\"_\",1)[0])\n",
    "        df_lbl = pd.read_csv(path,sep=r'\\s+')      \n",
    "        # print(\"Shape:\", df_lbl.shape)    \n",
    "        # print(\"Columns:\", df_lbl.columns.tolist())\n",
    "        # print(df_lbl.head())\n",
    "        # print(sid)\n",
    "        # # df_lbl = pd.read_csv(path)\n",
    "        df_lbl.columns = [\"time\",\"class\"]\n",
    "        labels[sid] = df_lbl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41cf5fba-aa3e-45eb-8f1d-53369232738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc565171-55aa-4b03-9db6-5df9d130bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"labels_csv/2598705_labeled_sleep.csv\"\n",
    "# with open(file_path, 'r') as f:\n",
    "#     for _ in range(5):\n",
    "#         print(f.readline())\n",
    "\n",
    "# df = pd.read_csv(file_path, sep=r'\\s+') \n",
    "\n",
    "# print(df.shape)      # shows (n_rows, n_columns)\n",
    "# print(df.columns)    # shows the column names pandas inferred\n",
    "# print(df.head(5))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b645e6ad-7d6e-473c-b0a7-506a6214b681",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########DEBUG MEN AWL HENAAAAAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7badc420-485f-4b3c-9053-0ba62b494d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_segments(df_stream, start, end, n_segments):\n",
    "    seg_len = (end - start) / n_segments #tab ma di dayman hatkoon constanst as end - start = 30 change????\n",
    "    seg_means = {}\n",
    "    \n",
    "    # Number of value columns \n",
    "    n_cols = df_stream.shape[1] - 1\n",
    "    \n",
    "    for j in range(n_segments):\n",
    "        lo, hi = start + j*seg_len, start + (j+1)*seg_len\n",
    "        \n",
    "        # Boolean mask on the 'time' column (first column)\n",
    "        mask = (df_stream.iloc[:, 0] >= lo) & (df_stream.iloc[:, 0] < hi)\n",
    "        for i in range(1, n_cols + 1):\n",
    "            vals = df_stream.loc[mask, df_stream.columns[i]]\n",
    "\n",
    "        for i in range(1, n_cols+1):\n",
    "            # select rows where mask is True, and the i-th column\n",
    "            vals = df_stream.loc[mask, df_stream.columns[i]]\n",
    "            \n",
    "            feat_name = f\"col{i}_seg{j+1}\"\n",
    "            seg_means[feat_name] = vals.mean()\n",
    "    \n",
    "    return seg_means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31bd316d-23d7-4389-998b-ed1850825c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_subject_matrix(sid, by_subject, labels, n_segments):\n",
    "    df_lbl = labels[sid]         \n",
    "    rows   = []                 \n",
    "\n",
    "    for start, cls in zip(df_lbl[\"time\"], df_lbl[\"class\"]):\n",
    "        end = start + 30.0\n",
    "        row = {\n",
    "            \"subject\":       sid,\n",
    "            \"window_start\":  start,\n",
    "            \"class\":         cls\n",
    "        }\n",
    "\n",
    "        for stream_name, df_stream in by_subject[sid].items():\n",
    "            seg_dict = aggregate_segments(df_stream, start, end, n_segments)\n",
    "            for feat, val in seg_dict.items():\n",
    "                row[f\"{stream_name}_{feat}\"] = val\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34791f81-851f-475b-baaa-89b6a44fe759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all_subjects_segmented.csv → shape (27180, 28)\n"
     ]
    }
   ],
   "source": [
    "def build_all_subjects(by_subject, labels, n_segments, out_csv):\n",
    "    all_dfs = []\n",
    "    for sid in sorted(labels.keys()):\n",
    "        df_subj = build_subject_matrix(sid, by_subject, labels, n_segments)\n",
    "        all_dfs.append(df_subj)\n",
    "\n",
    "    # 5.1 Combine vertically\n",
    "    full_df = pd.concat(all_dfs, axis=0, ignore_index=True)\n",
    "\n",
    "    # 5.2 Save to CSV\n",
    "    full_df.to_csv(out_csv, index=False)\n",
    "    return full_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    N = 5\n",
    "    OUT = \"all_subjects_segmented.csv\"\n",
    "\n",
    "    df_final = build_all_subjects(by_subject, labels, N, OUT)\n",
    "    print(f\"Saved {OUT} → shape {df_final.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
